{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNUQBPdWQQLZ4W5efaSgQBc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AdairGarcia/Semillero-IA-hitss/blob/main/Teoria/MatrizDeConfusion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Matriz de confusión\n"
      ],
      "metadata": {
        "id": "Gzoule6c2pTg"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "539b6919"
      },
      "source": [
        "Una **Matriz de Confusión** es una tabla que se utiliza para evaluar el rendimiento de un modelo de clasificación. Muestra un resumen de los resultados de la predicción de un modelo en comparación con los valores reales.\n",
        "\n",
        "Una matriz de confusión típica para un problema de clasificación binaria (dos clases) se ve así:\n",
        "\n",
        "|              | Predicción Positiva | Predicción Negativa |\n",
        "|--------------|----------------------|----------------------|\n",
        "| **Real Positivo** | Verdaderos Positivos (VP) | Falsos Negativos (FN) |\n",
        "| **Real Negativo** | Falsos Positivos (FP) | Verdaderos Negativos (VN) |\n",
        "\n",
        "Donde:\n",
        "\n",
        "*   **Verdaderos Positivos (VP):** Son las instancias que el modelo predijo como positivas y que realmente son positivas.\n",
        "*   **Falsos Negativos (FN):** Son las instancias que el modelo predijo como negativas pero que realmente son positivas. También conocidos como errores de Tipo II.\n",
        "*   **Falsos Positivos (FP):** Son las instancias que el modelo predijo como positivas pero que realmente son negativas. También conocidos como errores de Tipo I.\n",
        "*   **Verdaderos Negativos (VN):** Son las instancias que el modelo predijo como negativas y que realmente son negativas.\n",
        "\n",
        "A partir de estos valores, se pueden calcular diversas métricas de evaluación:\n",
        "\n",
        "*   **Precisión (Precision):** Mide la proporción de predicciones positivas que fueron correctas.\n",
        "\\begin{equation}\n",
        "     \\text{Precisión} = \\frac{VP}{VP + FP}\n",
        "\\end{equation}\n",
        "\n",
        "*   **Exhaustividad (Recall) / Sensibilidad (Sensitivity):** Mide la proporción de instancias positivas reales que fueron identificadas correctamente.\n",
        "    $$ \\text{Exhaustividad} = \\frac{VP}{VP + FN} $$\n",
        "\n",
        "*   **Especificidad (Specificity):** Mide la proporción de instancias negativas reales que fueron identificadas correctamente.\n",
        "    $$ \\text{Especificidad} = \\frac{VN}{VN + FP} $$\n",
        "\n",
        "*   **Exactitud (Accuracy):** Mide la proporción total de predicciones correctas (tanto positivas como negativas).\n",
        "    $$ \\text{Exactitud} = \\frac{VP + VN}{VP + VN + FP + FN} $$\n",
        "\n",
        "*   **Puntuación F1 (F1-Score):** Es la media armónica de la precisión y la exhaustividad, útil cuando hay un desequilibrio en las clases.\n",
        "    $$ \\text{Puntuación F1} = 2 \\times \\frac{\\text{Precisión} \\times \\text{Exhaustividad}}{\\text{Precisión} + \\text{Exhaustividad}} $$\n",
        "\n"
      ]
    }
  ]
}